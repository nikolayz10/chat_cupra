import json
import os
from typing import List, Dict, Any
from openai import OpenAI
from dotenv import load_dotenv
from services.llm.cupra_retrieval import busqueda_cupra_chunks, cupra_retriever

load_dotenv()

# Configuraci√≥n
OPENAI_API_KEY = os.getenv('KEY_OPENAI')
MODEL_GPT = os.getenv('MODEL_LLM', 'gpt-4o-mini')

# Configurar cliente OpenAI
client = OpenAI(api_key=OPENAI_API_KEY)

class CupraRAGPipeline:
    """Pipeline RAG completo para asistencia CUPRA: RAG ‚Üí LLM ‚Üí Quality Agent"""
    
    def __init__(self, logger=None):
        """Inicializa el pipeline verificando conexiones"""
        # print("üöÄ Inicializando CUPRA RAG Pipeline...")
        
        self.logger = logger
        
        # Verificar conexi√≥n a base de datos
        salud_bd = cupra_retriever.verificar_salud_bd()
        if not salud_bd['conexion_ok']:
            # self.logger.error("‚ùå No se puede conectar a la base de datos PostgreSQL")
            raise Exception("‚ùå No se puede conectar a la base de datos PostgreSQL")
        
        # Verificar que hay datos
        stats = cupra_retriever.obtener_estadisticas_bd()
        if stats.get('total_chunks', 0) == 0:
            # self.logger.error("‚ùå No hay chunks en la base de datos")
            raise Exception("‚ùå No hay chunks en la base de datos")
        
        # self.logger.info(f"‚úÖ Pipeline inicializado - {stats.get('total_chunks', 0)} chunks disponibles")
    
    def paso_1_rag(self, query: str, top_k: int = 4) -> List[Dict]:
        """
        PASO 1: RAG - Recuperaci√≥n de informaci√≥n relevante
        
        Args:
            query: Consulta del usuario
            top_k: N√∫mero de chunks a recuperar (default: 4)
            
        Returns:
            Lista de chunks m√°s relevantes con similitud coseno
        """
        # print(f"üîç PASO 1 - RAG: Buscando informaci√≥n para '{query}'...")
        self.logger.info(f"PASO 1 - RAG: Buscando informaci√≥n para '{query}'...")
        
        try:
            # Usar el sistema de b√∫squeda vectorial
            resultados = busqueda_cupra_chunks(query, top_k=top_k, logger = self.logger)
            
            if resultados:
                # print(f"‚úÖ RAG exitoso: {len(resultados)} chunks recuperados")
                self.logger.info(f" RAG exitoso: {len(resultados)} chunks recuperados")
                # for i, resultado in enumerate(resultados, 1):
                #     similitud_pct = resultado['similitud'] * 100
                #     titulo_short = resultado['titulo'][:50] + "..." if len(resultado['titulo']) > 50 else resultado['titulo']
                #     print(f"   {i}. {titulo_short} ({similitud_pct:.1f}%)")
            else:
                # print("‚ùå RAG: No se encontraron chunks relevantes")
                self.logger.error("‚ùå RAG: No se encontraron chunks relevantes")
                
            return resultados
            
        except Exception as e:
            # print(f"‚ùå Error en PASO 1 - RAG: {e}")
            self.logger.warning(f"‚ùå Error en PASO 1 - RAG: {e}")
            return []
    
    def paso_2_llm(self, query: str, chunks_relevantes: List[Dict]) -> Dict[str, Any]:
        """
        PASO 2: LLM - Generaci√≥n de respuesta con contexto
        
        Args:
            query: Consulta original del usuario
            chunks_relevantes: Chunks recuperados del RAG
            
        Returns:
            Diccionario con la respuesta generada y metadatos
        """
        # print(f"ü§ñ PASO 2 - LLM: Generando respuesta...")
        self.logger.info(f"PASO 2 - LLM: Generando respuesta...")
        
        if not chunks_relevantes:
            self.logger.error("No se encontro informaci√≥n relevante en el manual CUPRA")
            return {
                'respuesta': "Lo siento, no encontr√© informaci√≥n relevante en el manual de CUPRA para responder tu consulta. ¬øPodr√≠as reformular tu pregunta o ser m√°s espec√≠fico?",
                'contexto_usado': [],
                'confianza': 0.0,
                'fuentes': []
            }
        
        try:
            # Construir contexto desde los chunks
            self.logger.debug("Construyendo contexto desde los chunks")
            contexto = self._construir_contexto(chunks_relevantes)
            
            # Crear prompt especializado para CUPRA
            self.logger.debug("Creando prompt")
            prompt = self._crear_prompt_cupra(query, contexto)
            
            # Llamar al LLM
            self.logger.debug("haciendo llamada al llm")
            respuesta = client.chat.completions.create(
                model=MODEL_GPT,
                messages=[
                    {
                        "role": "system", 
                        "content": "Eres un asistente t√©cnico especializado en veh√≠culos CUPRA. Responde √öNICAMENTE bas√°ndote en la informaci√≥n del manual oficial proporcionada. Si no tienes informaci√≥n suficiente, ind√≠calo claramente. Mant√©n un tono profesional pero accesible."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                temperature=0.3,  # Baja temperatura para respuestas m√°s consistentes
                max_tokens=1000
            )
            
            respuesta_texto = respuesta.choices[0].message.content
            
            # Calcular confianza promedio basada en similitud de chunks
            confianza = sum(chunk['similitud'] for chunk in chunks_relevantes) / len(chunks_relevantes)
            
            # Extraer informaci√≥n de fuentes
            fuentes = [
                {
                    'titulo': chunk['titulo'],
                    'similitud': chunk['similitud'],
                    'subchunk': chunk['subchunk'],
                    'chunk_id': chunk['chunk_id']
                }
                for chunk in chunks_relevantes
            ]
            
            resultado = {
                'respuesta': respuesta_texto,
                'contexto_usado': contexto,
                'confianza': confianza,
                'fuentes': fuentes
            }
            
            # print(f"‚úÖ LLM: Respuesta generada (Confianza: {confianza:.2f})")
            self.logger.info(f" LLM Confianza: {confianza:.2f}")
            return resultado
            
        except Exception as e:
            # print(f"‚ùå Error en PASO 2 - LLM: {e}")
            self.logger.warning(f"‚ùå Error en PASO 2 - LLM: {e}")
            return {
                'respuesta': f"Error generando respuesta: {str(e)}",
                'contexto_usado': [],
                'confianza': 0.0,
                'fuentes': []
            }
    
    def paso_3_quality_agent(self, query: str, respuesta_llm: Dict[str, Any]) -> str:
        """
        PASO 3: Quality Agent - Evaluaci√≥n de calidad de la respuesta
        
        Args:
            query: Consulta original
            respuesta_llm: Respuesta del LLM con metadatos
            
        Returns:
            Puntuaci√≥n de calidad (1-10)
        """
        # print(f"üîç PASO 3 - Quality Agent: Evaluando calidad...")
        self.logger.info(f"PASO 3 - Quality Agent: Evaluando calidad...")
        
        try:
            # Crear prompt para evaluaci√≥n de calidad
            prompt_quality = self._crear_prompt_quality(query, respuesta_llm)
            
            # Evaluar calidad
            evaluacion = client.chat.completions.create(
                model=MODEL_GPT,
                messages=[
                    {
                        "role": "system", 
                        "content": "Eres un evaluador de calidad de respuestas t√©cnicas. Eval√∫a la respuesta del 1 al 10 considerando: relevancia, precisi√≥n, completitud, claridad y fundamentaci√≥n. Responde SOLO con el n√∫mero."
                    },
                    {
                        "role": "user", 
                        "content": prompt_quality
                    }
                ],
                temperature=0.1,
                max_tokens=10
            )
            
            evaluacion_texto = evaluacion.choices[0].message.content.strip()
            
            # Extraer n√∫mero de la evaluaci√≥n
            try:
                puntuacion = int(evaluacion_texto)
                if 1 <= puntuacion <= 10:
                    # print(f"‚úÖ Quality Agent: Puntuaci√≥n {puntuacion}/10")
                    self.logger.info(f"Quality Agent: Puntuaci√≥n {puntuacion}/10")
                    return str(puntuacion)
                else:
                    # print(f"‚ö†Ô∏è Quality Agent: Puntuaci√≥n fuera de rango, usando 5")
                    self.logger.error(f"‚ö†Ô∏è Quality Agent: Puntuaci√≥n fuera de rango, usando 5")
                    return "5"
            except:
                # print(f"‚ö†Ô∏è Quality Agent: No se pudo extraer puntuaci√≥n, usando 5")
                self.logger.error(f"‚ö†Ô∏è Quality Agent: No se pudo extraer puntuaci√≥n, usando 5")
                return "5"
            
        except Exception as e:
            print(f"‚ùå Error en PASO 3 - Quality Agent: {e}")
            self.logger.warning(f"Error en PASO 3 - Quality Agent: {e}")
            return "5"  # Puntuaci√≥n por defecto
    
    def _construir_contexto(self, chunks: List[Dict]) -> List[str]:
        """Construye el contexto a partir de los chunks relevantes"""
        contexto_partes = []
        
        for i, chunk in enumerate(chunks, 1):
            contexto_parte = f"INFORMACI√ìN {i}:\n"
            contexto_parte += f"T√≠tulo: {chunk['titulo']}\n"
            if chunk['subchunk'] > 0:
                contexto_parte += f"Secci√≥n: {chunk['subchunk']}\n"
            contexto_parte += f"Contenido: {chunk['cont']}\n"
            contexto_partes.append(contexto_parte)
        
        return contexto_partes
    
    def _crear_prompt_cupra(self, query: str, contexto: List[str]) -> str:
        """Crea el prompt especializado para CUPRA"""
        contexto_str = "\n\n".join(contexto)
        
        prompt = f"""Bas√°ndote EXCLUSIVAMENTE en la siguiente informaci√≥n del manual oficial de CUPRA, responde la consulta del usuario de manera precisa y √∫til.

INFORMACI√ìN DEL MANUAL CUPRA:
{contexto_str}

CONSULTA DEL USUARIO:
{query}

INSTRUCCIONES DE FORMATO Y CONTENIDO:
1. Responde √öNICAMENTE bas√°ndote en la informaci√≥n proporcionada del manual
2. IMPORTANTE - Estructura tu respuesta de forma clara y legible:
   - Usa SALTOS DE L√çNEA entre secciones principales
   - Para pasos numerados, pon cada paso en una nueva l√≠nea
   - Separa claramente las secciones (ej: pasos principales, condiciones, precauciones)
   - Usa **negrita** para t√≠tulos de secciones importantes
   - Deja una l√≠nea en blanco entre p√°rrafos diferentes
3. Si hay procedimientos paso a paso:
   - Numera cada paso principal (1., 2., 3., etc.)
   - Los sub-pasos van con guiones (-)
   - Cada paso en su propia l√≠nea
4. Agrupa la informaci√≥n en secciones l√≥gicas como:
   - Pasos principales
   - Condiciones importantes
   - Precauciones o advertencias
   - Informaci√≥n adicional
5. Mant√©n un tono profesional pero accesible
6. Si la informaci√≥n no es suficiente, ind√≠calo claramente al final

FORMATO DE EJEMPLO para respuestas con pasos:

**T√≠tulo de la funci√≥n**

**Pasos principales:**

1. **Primer paso**
   - Detalle del paso
   - Otro detalle si es necesario

2. **Segundo paso**
   - Explicaci√≥n clara

**Condiciones importantes:**
- Primera condici√≥n
- Segunda condici√≥n

**Precauciones:**
‚ö†Ô∏è Advertencia importante

RESPUESTA:"""
        
        return prompt
    
    def _crear_prompt_quality(self, query: str, respuesta_llm: Dict[str, Any]) -> str:
        """Crea el prompt para evaluaci√≥n de calidad"""
        prompt = f"""Eval√∫a del 1 al 10 la calidad de esta respuesta sobre veh√≠culos CUPRA:

CONSULTA:
{query}

RESPUESTA:
{respuesta_llm['respuesta']}

CRITERIOS DE EVALUACI√ìN:
1. Relevancia: ¬øResponde directamente a la consulta?
2. Precisi√≥n: ¬øEs t√©cnicamente correcta?
3. Completitud: ¬øCubre los aspectos importantes?
4. Claridad: ¬øEs f√°cil de entender?
5. Fundamentaci√≥n: ¬øEst√° basada en la informaci√≥n proporcionada?

Responde SOLO con el n√∫mero del 1 al 10:"""
        
        return prompt
    
    def procesar_consulta_completa(self, query: str) -> Dict[str, Any]:
        """
        Ejecuta el pipeline completo: RAG ‚Üí LLM ‚Üí Quality Agent
        
        Args:
            query: Consulta del usuario
            
        Returns:
            Diccionario con todos los resultados del pipeline
        """
        # print(f"\n{'='*60}")
        # print(f"üöÄ CUPRA RAG PIPELINE INICIADO")
        # print(f"üîç Consulta: '{query}'")
        # print(f"{'='*60}")
        
        self.logger.info("CUPRA RAG PIPELINE INICIADO")
        
        # Paso 1: RAG - Recuperaci√≥n
        chunks_relevantes = self.paso_1_rag(query, top_k=4)
        
        # Paso 2: LLM - Generaci√≥n
        respuesta_llm = self.paso_2_llm(query, chunks_relevantes)
        
        # Paso 3: Quality Agent - Evaluaci√≥n
        evaluacion_calidad = self.paso_3_quality_agent(query, respuesta_llm)
        
        # Resultado final
        resultado_final = {
            'query': query,
            'chunks_recuperados': chunks_relevantes,
            'respuesta_llm': respuesta_llm,
            'evaluacion_calidad': evaluacion_calidad,
            'timestamp': self._get_timestamp(),
            'fuente': 'PostgreSQL'
        }
        
        # Mostrar resumen
        # self._mostrar_resumen_final(resultado_final)
        
        return resultado_final
    
    def _get_timestamp(self) -> str:
        """Obtiene timestamp actual"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def _mostrar_resumen_final(self, resultado: Dict[str, Any]):
        """Muestra un resumen del procesamiento completo"""
        print(f"\n{'='*60}")
        print(f"üìã RESUMEN FINAL - CUPRA RAG PIPELINE")
        print(f"{'='*60}")
        
        print(f"üìä Chunks recuperados: {len(resultado['chunks_recuperados'])}")
        print(f"ü§ñ Respuesta generada: {len(resultado['respuesta_llm']['respuesta'])} caracteres")
        print(f"‚≠ê Confianza: {resultado['respuesta_llm']['confianza']:.2f}")
        print(f"üèÜ Puntuaci√≥n calidad: {resultado['evaluacion_calidad']}/10")
        print(f"üíæ Fuente: {resultado['fuente']}")
        
        print(f"\nüîç RESPUESTA FINAL:")
        print(f"{'‚îÄ'*40}")
        print(resultado['respuesta_llm']['respuesta'])
        print(f"{'‚îÄ'*40}")

def main():
    """Funci√≥n principal para probar el pipeline"""
    try:
        # Verificar configuraci√≥n
        if not OPENAI_API_KEY:
            print("‚ùå Error: Configura tu API key de OpenAI")
            return
        
        # Inicializar pipeline
        pipeline = CupraRAGPipeline()
        
        print("üöÄ CUPRA RAG Pipeline iniciado")
        print("üí° Ejemplos de consultas:")
        print("   - '¬øC√≥mo funciona el sistema de luces del CUPRA?'")
        print("   - '¬øQu√© tipos de airbags tiene el veh√≠culo?'")
        print("   - '¬øC√≥mo se usa la climatizaci√≥n?'")
        print("   - '¬øCu√°les son las caracter√≠sticas de la c√°mara frontal?'")
        
        while True:
            # Obtener consulta del usuario
            query = input("\nüîç Ingresa tu consulta (o 'salir' para terminar): ").strip()
            
            if query.lower() in ['salir', 'exit', 'quit', 'q']:
                print("üëã ¬°Hasta luego!")
                break
            
            if not query:
                print("‚ö†Ô∏è Por favor ingresa una consulta v√°lida")
                continue
            
            # Procesar consulta completa
            resultado = pipeline.procesar_consulta_completa(query)
            
            # Opcional: Guardar resultado
            with open("resultado_cupra.json", 'w', encoding='utf-8') as f:
                json.dump(resultado, f, ensure_ascii=False, indent=2)
            
    except KeyboardInterrupt:
        print("\nüëã ¬°Proceso cancelado!")
    except Exception as e:
        print(f"‚ùå Error en el proceso principal: {e}")

if __name__ == "__main__":
    main()