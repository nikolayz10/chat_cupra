import json
import os
from typing import List, Dict, Any
from openai import OpenAI
from dotenv import load_dotenv
from services.llm.cupra_retrieval import busqueda_cupra_chunks, cupra_retriever

load_dotenv()

# ConfiguraciÃ³n
OPENAI_API_KEY = os.getenv('KEY_OPENAI')
MODEL_GPT = os.getenv('MODEL_LLM', 'gpt-4o-mini')

# Configurar cliente OpenAI
client = OpenAI(api_key=OPENAI_API_KEY)

class CupraRAGPipeline:
    """Pipeline RAG completo para asistencia CUPRA: RAG â†’ LLM â†’ Quality Agent"""
    
    def __init__(self, logger=None):
        """Inicializa el pipeline verificando conexiones"""
        # print("ğŸš€ Inicializando CUPRA RAG Pipeline...")
        
        self.logger = logger
        
        # Verificar conexiÃ³n a base de datos
        salud_bd = cupra_retriever.verificar_salud_bd()
        if not salud_bd['conexion_ok']:
            # self.logger.error("âŒ No se puede conectar a la base de datos PostgreSQL")
            raise Exception("âŒ No se puede conectar a la base de datos PostgreSQL")
        
        # Verificar que hay datos
        stats = cupra_retriever.obtener_estadisticas_bd()
        if stats.get('total_chunks', 0) == 0:
            # self.logger.error("âŒ No hay chunks en la base de datos")
            raise Exception("âŒ No hay chunks en la base de datos")
        
        # self.logger.info(f"âœ… Pipeline inicializado - {stats.get('total_chunks', 0)} chunks disponibles")
    
    def paso_1_rag(self, query: str, top_k: int = 4) -> List[Dict]:
        """
        PASO 1: RAG - RecuperaciÃ³n de informaciÃ³n relevante
        
        Args:
            query: Consulta del usuario
            top_k: NÃºmero de chunks a recuperar (default: 4)
            
        Returns:
            Lista de chunks mÃ¡s relevantes con similitud coseno
        """
        # print(f"ğŸ” PASO 1 - RAG: Buscando informaciÃ³n para '{query}'...")
        self.logger.info(f"PASO 1 - RAG: Buscando informaciÃ³n para '{query}'...")
        
        try:
            # Usar el sistema de bÃºsqueda vectorial
            resultados = busqueda_cupra_chunks(query, top_k=top_k, logger = self.logger)
            
            if resultados:
                # print(f"âœ… RAG exitoso: {len(resultados)} chunks recuperados")
                self.logger.info(f" RAG exitoso: {len(resultados)} chunks recuperados")
                # for i, resultado in enumerate(resultados, 1):
                #     similitud_pct = resultado['similitud'] * 100
                #     titulo_short = resultado['titulo'][:50] + "..." if len(resultado['titulo']) > 50 else resultado['titulo']
                #     print(f"   {i}. {titulo_short} ({similitud_pct:.1f}%)")
            else:
                # print("âŒ RAG: No se encontraron chunks relevantes")
                self.logger.error("âŒ RAG: No se encontraron chunks relevantes")
                
            return resultados
            
        except Exception as e:
            # print(f"âŒ Error en PASO 1 - RAG: {e}")
            self.logger.warning(f"âŒ Error en PASO 1 - RAG: {e}")
            return []
    
    def paso_2_llm(self, query: str, chunks_relevantes: List[Dict]) -> Dict[str, Any]:
        """
        PASO 2: LLM - GeneraciÃ³n de respuesta con contexto
        
        Args:
            query: Consulta original del usuario
            chunks_relevantes: Chunks recuperados del RAG
            
        Returns:
            Diccionario con la respuesta generada y metadatos
        """
        # print(f"ğŸ¤– PASO 2 - LLM: Generando respuesta...")
        self.logger.info(f"PASO 2 - LLM: Generando respuesta...")
        
        if not chunks_relevantes:
            self.logger.error("No se encontro informaciÃ³n relevante en el manual CUPRA")
            return {
                'respuesta': "Lo siento, no encontrÃ© informaciÃ³n relevante en el manual de CUPRA para responder tu consulta. Â¿PodrÃ­as reformular tu pregunta o ser mÃ¡s especÃ­fico?",
                'contexto_usado': [],
                'confianza': 0.0,
                'fuentes': []
            }
        
        try:
            # Construir contexto desde los chunks
            self.logger.debug("Construyendo contexto desde los chunks")
            contexto = self._construir_contexto(chunks_relevantes)
            
            # Crear prompt especializado para CUPRA
            self.logger.debug("Creando prompt")
            prompt = self._crear_prompt_cupra(query, contexto)
            
            # Llamar al LLM
            self.logger.debug("haciendo llamada al llm")
            respuesta = client.chat.completions.create(
                model=MODEL_GPT,
                messages=[
                    {
                        "role": "system", 
                        "content": "Eres un asistente tÃ©cnico especializado en vehÃ­culos CUPRA. Responde ÃšNICAMENTE basÃ¡ndote en la informaciÃ³n del manual oficial proporcionada. Si no tienes informaciÃ³n suficiente, indÃ­calo claramente. MantÃ©n un tono profesional pero accesible."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                temperature=0.3,  # Baja temperatura para respuestas mÃ¡s consistentes
                max_tokens=1000
            )
            
            respuesta_texto = respuesta.choices[0].message.content
            
            # Calcular confianza promedio basada en similitud de chunks
            confianza = sum(chunk['similitud'] for chunk in chunks_relevantes) / len(chunks_relevantes)
            
            # Extraer informaciÃ³n de fuentes
            fuentes = [
                {
                    'titulo': chunk['titulo'],
                    'similitud': chunk['similitud'],
                    'subchunk': chunk['subchunk'],
                    'chunk_id': chunk['chunk_id']
                }
                for chunk in chunks_relevantes
            ]
            
            resultado = {
                'respuesta': respuesta_texto,
                'contexto_usado': contexto,
                'confianza': confianza,
                'fuentes': fuentes
            }
            
            # print(f"âœ… LLM: Respuesta generada (Confianza: {confianza:.2f})")
            self.logger.info(f" LLM Confianza: {confianza:.2f}")
            return resultado
            
        except Exception as e:
            # print(f"âŒ Error en PASO 2 - LLM: {e}")
            self.logger.warning(f"âŒ Error en PASO 2 - LLM: {e}")
            return {
                'respuesta': f"Error generando respuesta: {str(e)}",
                'contexto_usado': [],
                'confianza': 0.0,
                'fuentes': []
            }
    
    def paso_3_quality_agent(self, query: str, respuesta_llm: Dict[str, Any]) -> str:
        """
        PASO 3: Quality Agent - EvaluaciÃ³n de calidad de la respuesta
        
        Args:
            query: Consulta original
            respuesta_llm: Respuesta del LLM con metadatos
            
        Returns:
            PuntuaciÃ³n de calidad (1-10)
        """
        # print(f"ğŸ” PASO 3 - Quality Agent: Evaluando calidad...")
        self.logger.info(f"PASO 3 - Quality Agent: Evaluando calidad...")
        
        try:
            # Crear prompt para evaluaciÃ³n de calidad
            prompt_quality = self._crear_prompt_quality(query, respuesta_llm)
            
            # Evaluar calidad
            evaluacion = client.chat.completions.create(
                model=MODEL_GPT,
                messages=[
                    {
                        "role": "system", 
                        "content": "Eres un evaluador de calidad de respuestas tÃ©cnicas. EvalÃºa la respuesta del 1 al 10 considerando: relevancia, precisiÃ³n, completitud, claridad y fundamentaciÃ³n. Responde SOLO con el nÃºmero."
                    },
                    {
                        "role": "user", 
                        "content": prompt_quality
                    }
                ],
                temperature=0.1,
                max_tokens=10
            )
            
            evaluacion_texto = evaluacion.choices[0].message.content.strip()
            
            # Extraer nÃºmero de la evaluaciÃ³n
            try:
                puntuacion = int(evaluacion_texto)
                if 1 <= puntuacion <= 10:
                    # print(f"âœ… Quality Agent: PuntuaciÃ³n {puntuacion}/10")
                    self.logger.info(f"Quality Agent: PuntuaciÃ³n {puntuacion}/10")
                    return str(puntuacion)
                else:
                    # print(f"âš ï¸ Quality Agent: PuntuaciÃ³n fuera de rango, usando 5")
                    self.logger.error(f"âš ï¸ Quality Agent: PuntuaciÃ³n fuera de rango, usando 5")
                    return "5"
            except:
                # print(f"âš ï¸ Quality Agent: No se pudo extraer puntuaciÃ³n, usando 5")
                self.logger.error(f"âš ï¸ Quality Agent: No se pudo extraer puntuaciÃ³n, usando 5")
                return "5"
            
        except Exception as e:
            print(f"âŒ Error en PASO 3 - Quality Agent: {e}")
            self.logger.warning(f"Error en PASO 3 - Quality Agent: {e}")
            return "5"  # PuntuaciÃ³n por defecto
    
    def _construir_contexto(self, chunks: List[Dict]) -> List[str]:
        """Construye el contexto a partir de los chunks relevantes"""
        contexto_partes = []
        
        for i, chunk in enumerate(chunks, 1):
            contexto_parte = f"INFORMACIÃ“N {i}:\n"
            contexto_parte += f"TÃ­tulo: {chunk['titulo']}\n"
            if chunk['subchunk'] > 0:
                contexto_parte += f"SecciÃ³n: {chunk['subchunk']}\n"
            contexto_parte += f"Contenido: {chunk['cont']}\n"
            contexto_partes.append(contexto_parte)
        
        return contexto_partes
    
    def _crear_prompt_cupra(self, query: str, contexto: List[str]) -> str:
        """Crea el prompt especializado para CUPRA"""
        contexto_str = "\n\n".join(contexto)
        
        prompt = f"""BasÃ¡ndote EXCLUSIVAMENTE en la siguiente informaciÃ³n del manual oficial de CUPRA, responde la consulta del usuario de manera precisa y Ãºtil.

INFORMACIÃ“N DEL MANUAL CUPRA:
{contexto_str}

CONSULTA DEL USUARIO:
{query}

INSTRUCCIONES DE FORMATO Y CONTENIDO:
1. Responde ÃšNICAMENTE basÃ¡ndote en la informaciÃ³n proporcionada del manual
2. IMPORTANTE - Estructura tu respuesta de forma clara y legible:
   - Usa SALTOS DE LÃNEA entre secciones principales
   - Para pasos numerados, pon cada paso en una nueva lÃ­nea
   - Separa claramente las secciones (ej: pasos principales, condiciones, precauciones)
   - Usa **negrita** para tÃ­tulos de secciones importantes
   - Deja una lÃ­nea en blanco entre pÃ¡rrafos diferentes
3. Si hay procedimientos paso a paso:
   - Numera cada paso principal (1., 2., 3., etc.)
   - Los sub-pasos van con guiones (-)
   - Cada paso en su propia lÃ­nea
4. Agrupa la informaciÃ³n en secciones lÃ³gicas como:
   - Pasos principales
   - Condiciones importantes
   - Precauciones o advertencias
   - InformaciÃ³n adicional
5. MantÃ©n un tono profesional pero accesible
6. Si la informaciÃ³n no es suficiente, indÃ­calo claramente al final

FORMATO DE EJEMPLO para respuestas con pasos:

**TÃ­tulo de la funciÃ³n**

**Pasos principales:**

1. **Primer paso**
   - Detalle del paso
   - Otro detalle si es necesario

2. **Segundo paso**
   - ExplicaciÃ³n clara

**Condiciones importantes:**
- Primera condiciÃ³n
- Segunda condiciÃ³n

**Precauciones:**
âš ï¸ Advertencia importante

RESPUESTA:"""
        
        return prompt
    
    def _crear_prompt_quality(self, query: str, respuesta_llm: Dict[str, Any]) -> str:
        """Crea el prompt para evaluaciÃ³n de calidad"""
        prompt = f"""EvalÃºa del 1 al 10 la calidad de esta respuesta sobre vehÃ­culos CUPRA:

CONSULTA:
{query}

RESPUESTA:
{respuesta_llm['respuesta']}

CRITERIOS DE EVALUACIÃ“N:
1. Relevancia: Â¿Responde directamente a la consulta?
2. PrecisiÃ³n: Â¿Es tÃ©cnicamente correcta?
3. Completitud: Â¿Cubre los aspectos importantes?
4. Claridad: Â¿Es fÃ¡cil de entender?
5. FundamentaciÃ³n: Â¿EstÃ¡ basada en la informaciÃ³n proporcionada?

Responde SOLO con el nÃºmero del 1 al 10:"""
        
        return prompt
    
    def procesar_consulta_completa(self, query: str) -> Dict[str, Any]:
        """
        Ejecuta el pipeline completo: RAG â†’ LLM â†’ Quality Agent
        
        Args:
            query: Consulta del usuario
            
        Returns:
            Diccionario con todos los resultados del pipeline
        """
        # print(f"\n{'='*60}")
        # print(f"ğŸš€ CUPRA RAG PIPELINE INICIADO")
        # print(f"ğŸ” Consulta: '{query}'")
        # print(f"{'='*60}")
        
        self.logger.info("CUPRA RAG PIPELINE INICIADO")
        
        # Paso 1: RAG - RecuperaciÃ³n
        chunks_relevantes = self.paso_1_rag(query, top_k=4)
        
        # Paso 2: LLM - GeneraciÃ³n
        respuesta_llm = self.paso_2_llm(query, chunks_relevantes)
        
        # Paso 3: Quality Agent - EvaluaciÃ³n
        evaluacion_calidad = self.paso_3_quality_agent(query, respuesta_llm)
        
        # Resultado final
        resultado_final = {
            'query': query,
            'chunks_recuperados': chunks_relevantes,
            'respuesta_llm': respuesta_llm,
            'evaluacion_calidad': evaluacion_calidad,
            'timestamp': self._get_timestamp(),
            'fuente': 'PostgreSQL'
        }
        
        # Mostrar resumen
        # self._mostrar_resumen_final(resultado_final)
        
        return resultado_final
    
    def _get_timestamp(self) -> str:
        """Obtiene timestamp actual"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def _mostrar_resumen_final(self, resultado: Dict[str, Any]):
        """Muestra un resumen del procesamiento completo"""
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ RESUMEN FINAL - CUPRA RAG PIPELINE")
        print(f"{'='*60}")
        
        print(f"ğŸ“Š Chunks recuperados: {len(resultado['chunks_recuperados'])}")
        print(f"ğŸ¤– Respuesta generada: {len(resultado['respuesta_llm']['respuesta'])} caracteres")
        print(f"â­ Confianza: {resultado['respuesta_llm']['confianza']:.2f}")
        print(f"ğŸ† PuntuaciÃ³n calidad: {resultado['evaluacion_calidad']}/10")
        print(f"ğŸ’¾ Fuente: {resultado['fuente']}")
        
        print(f"\nğŸ” RESPUESTA FINAL:")
        print(f"{'â”€'*40}")
        print(resultado['respuesta_llm']['respuesta'])
        print(f"{'â”€'*40}")

def main():
    """FunciÃ³n principal para probar el pipeline"""
    try:
        # Verificar configuraciÃ³n
        if not OPENAI_API_KEY:
            print("âŒ Error: Configura tu API key de OpenAI")
            return
        
        # Inicializar pipeline
        pipeline = CupraRAGPipeline()
        
        print("ğŸš€ CUPRA RAG Pipeline iniciado")
        print("ğŸ’¡ Ejemplos de consultas:")
        print("   - 'Â¿CÃ³mo funciona el sistema de luces del CUPRA?'")
        print("   - 'Â¿QuÃ© tipos de airbags tiene el vehÃ­culo?'")
        print("   - 'Â¿CÃ³mo se usa la climatizaciÃ³n?'")
        print("   - 'Â¿CuÃ¡les son las caracterÃ­sticas de la cÃ¡mara frontal?'")
        
        while True:
            # Obtener consulta del usuario
            query = input("\nğŸ” Ingresa tu consulta (o 'salir' para terminar): ").strip()
            
            if query.lower() in ['salir', 'exit', 'quit', 'q']:
                print("ğŸ‘‹ Â¡Hasta luego!")
                break
            
            if not query:
                print("âš ï¸ Por favor ingresa una consulta vÃ¡lida")
                continue
            
            # Procesar consulta completa
            resultado = pipeline.procesar_consulta_completa(query)
            
            # Opcional: Guardar resultado
            with open("resultado_cupra.json", 'w', encoding='utf-8') as f:
                json.dump(resultado, f, ensure_ascii=False, indent=2)
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Â¡Proceso cancelado!")
    except Exception as e:
        print(f"âŒ Error en el proceso principal: {e}")

if __name__ == "__main__":
    main()